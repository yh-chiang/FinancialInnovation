{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2:Use LSTM & CNN model to classify MNIST dataset with at least 90%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.程式碼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* mnist_LSTM&CNN_training_r09723032_江奕萱.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import keras\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPool2D, Dropout, Flatten\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "'''\n",
    "訓練模型:\n",
    "先設定優化器Adam\n",
    "再來設定模型的Loss函數、優化器以及用來判斷模型好壞的依據（metrics）\n",
    "最後訓練模型\n",
    "'''\n",
    "def trainning(model, x_train, y_train, x_test, y_test, \n",
    "              learning_rate, training_iters, batch_size):\n",
    "    #設定優化器\n",
    "    adam = Adam(lr=learning_rate)\n",
    "    model.summary()\n",
    "    # 設定模型的Loss函數、優化器以及用來判斷模型好壞的依據（metrics）\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    #訓練模型\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size, epochs=training_iters,\n",
    "              verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "'''\n",
    "辨別機器學習模型的好壞，印出Confusion Matrix：\n",
    "先得到模型的預測結果\n",
    "然後與真正的答案做對照\n",
    "由左上至右下的對角線，代表True Positive和True Negative，\n",
    "亦即模型預測成功的狀況，因此數字越高越好\n",
    "'''\n",
    "\n",
    "def print_confusion_result(x_train, x_test, y_train, y_test, model):\n",
    "    # get train & test predictions\n",
    "    train_pred = model.predict_classes(x_train)\n",
    "    test_pred = model.predict_classes(x_test)\n",
    "    \n",
    "    # get train & test true labels\n",
    "    train_label = y_train\n",
    "    test_label =  y_test\n",
    "    \n",
    "    # confusion matrix\n",
    "    train_result_cm = confusion_matrix(train_label, train_pred, labels=range(10))\n",
    "    test_result_cm = confusion_matrix(test_label, test_pred, labels=range(10))\n",
    "    print(train_result_cm, '\\n'*2, test_result_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1)Use LSTM to classify MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#處理資料\n",
    "\n",
    "MNIST的資料是一個28*28的影像，這裡把他看成一行行的序列（28維度（28長的sequence）*28行）\n",
    "\n",
    "x 全部都是圖片的資料，\n",
    "分成 Training data 跟 Test data；\n",
    "y 全部都是 one-hot encoding 的 Label，\n",
    "代表著圖片裡的數字是多少，\n",
    "同樣也是分 Training data 跟 Test data。\n",
    "\n",
    "標準化數據:\n",
    "如果要對影像標準化，\n",
    "因為其中的每一個像素其值都是在0~255之間的一個數字，\n",
    "所以我們只要把每一個像素的值都除以255就可以讓所有數字被收斂到0~1之間，\n",
    "完成標準化\n",
    "'''\n",
    "# Mnist Dataset\n",
    "def lstm_preprocess(x_train, x_test, y_train, y_test, n_step, n_input, n_classes):\n",
    "    x_train = x_train.reshape(-1, n_step, n_input) #從一維的784像素轉換成二維的28*28\n",
    "    x_test = x_test.reshape(-1, n_step, n_input)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    #標準化數據\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, n_classes)\n",
    "    return (x_train, x_test, y_train, y_test)\n",
    "\n",
    "'''\n",
    "建立LSTM模型:\n",
    "主要是用一層LSTM以及兩層的dense進行預測，\n",
    "使用softmax作為計算的fuction\n",
    "\n",
    "模型調整:\n",
    "dense:增加一層\n",
    "'''\n",
    "def lstm_model(n_input, n_step, n_hidden, n_classes):\n",
    "    #添加LSTM、Dense層\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_hidden, batch_input_shape=(None, n_step, n_input), unroll=True))\n",
    "    model.add(Dense(84, activation='relu'))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "'''\n",
    "參數調整:\n",
    "batch_size:256->128\n",
    "training_iters = 1->3 \n",
    "'''\n",
    "\n",
    "def mnist_lstm_main():\n",
    "    # training parameters \n",
    "    learning_rate = 0.001 #學習率\n",
    "    training_iters = 3 \n",
    "    batch_size = 128 # BATCH的大小，相當於一次處理image的個數\n",
    "\n",
    "    # model parameters  神經網路的參數\n",
    "    n_input = 28  # x_i 的向量長度，image有28列(輸入一行，一行有28個數據)\n",
    "    n_step = 28    # 一個LSTM中，輸入序列的長度，image有28行\n",
    "    n_hidden = 256 #隱含層的特徵數\n",
    "    n_classes = 10 #輸出的數量，因為是分類問題，0~9個數字，這裡一共有10個\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test, y_train_o, y_test_o = lstm_preprocess(x_train, x_test, y_train, y_test, n_step, n_input, n_classes)\n",
    "\n",
    "    model = lstm_model(n_input, n_step, n_hidden, n_classes)\n",
    "    trainning(model, x_train, y_train_o, x_test, y_test_o, learning_rate, training_iters, batch_size)\n",
    "     #驗證模型\n",
    "    scores = model.evaluate(x_test, y_test_o, verbose=0)\n",
    "    print('LSTM test accuracy:', scores[1])\n",
    "    print_confusion_result(x_train, x_test, y_train, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2)Use CNN to classify MNIST dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "處理資料\n",
    "x 全部都是圖片的資料，\n",
    "分成 Training data 跟 Test data；\n",
    "y 全部都是 one-hot encoding 的 Label，\n",
    "代表著圖片裡的數字是多少，\n",
    "同樣也是分 Training data 跟 Test data。\n",
    "'''\n",
    "# Mnist Dataset\n",
    "def cnn_preprocess(x_train, x_test, y_train, y_test):\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255 #實證表示除以255之後效果會變好\n",
    "    x_test /= 255\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = keras.utils.to_categorical(y_test, 10)\n",
    "    return (x_train, x_test, y_train, y_test)\n",
    "\n",
    "'''\n",
    "建立cnn模型:\n",
    "先建立 Convolution 第一層，\n",
    "然後用 MaxPool 層簡化圖片像素，\n",
    "重複以上動作一次，\n",
    "接著用 Flattern 攤平維度，\n",
    "然後接 Dense 全連接層三層，\n",
    "最後輸出那10個類別\n",
    "'''\n",
    "# Model Structure\n",
    "def cnn_model():\n",
    "    # initializing CNN\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPool2D(strides=2))\n",
    "    # Second convolutional layer\n",
    "    model.add(Conv2D(filters=48, kernel_size=(5,5), padding='valid', activation='relu'))\n",
    "    model.add(MaxPool2D(strides=2))\n",
    "    #將 feature maps 攤平放入一個向量中\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(84, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "'''\n",
    "參數調整:\n",
    "batch_size:256->128\n",
    "training_iters = 1->3 \n",
    "'''\n",
    "def mnist_cnn_main():\n",
    "    # training parameters\n",
    "    learning_rate = 0.001  #學習率\n",
    "    training_iters = 3 #迭代次數\n",
    "    batch_size = 64\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test, y_train_o, y_test_o = cnn_preprocess(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    model = cnn_model()\n",
    "    trainning(model, x_train, y_train_o, x_test, y_test_o, learning_rate, training_iters, batch_size)\n",
    "    #驗證模型準確度\n",
    "    scores = model.evaluate(x_test, y_test_o, verbose=0)\n",
    "    print('CNN test accuracy:', scores[1])\n",
    "    print_confusion_result(x_train, x_test, y_train, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.執行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1)LSTM ，印出預測準確度以及Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 256)               291840    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 84)                21588     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 314,278\n",
      "Trainable params: 314,278\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 30s 507us/step - loss: 0.4524 - acc: 0.8477 - val_loss: 0.1636 - val_acc: 0.9486\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 21s 355us/step - loss: 0.1281 - acc: 0.9601 - val_loss: 0.1014 - val_acc: 0.9674\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 22s 363us/step - loss: 0.0854 - acc: 0.9737 - val_loss: 0.1003 - val_acc: 0.9701\n",
      "LSTM test accuracy: 0.9701\n",
      "[[5827    0    7    0    3    2   63    0   21    0]\n",
      " [   0 6536   37    6    7    3    8   21  110   14]\n",
      " [   9    2 5838    7    7    6    9    5   75    0]\n",
      " [   1    2   66 5880    1   42    1   10  104   24]\n",
      " [   2    0    0    0 5786    1   37    3    3   10]\n",
      " [   4    1    4    6   11 5338   28    0   14   15]\n",
      " [   3    0    0    0    6   12 5888    0    9    0]\n",
      " [   1    3   89   14   19    1    0 5951   64  123]\n",
      " [   5    3    3    2   19   16    8    0 5779   16]\n",
      " [  15    1    4    4  229   20    3    8   30 5635]] \n",
      "\n",
      " [[ 962    0    1    0    1    2   12    0    1    1]\n",
      " [   0 1105    4    2    0    0    1    1   22    0]\n",
      " [   3    0 1009    1    1    0    4    0   14    0]\n",
      " [   0    0    7  983    0    7    0    3    9    1]\n",
      " [   0    0    1    0  963    1   11    2    2    2]\n",
      " [   3    0    0    4    1  869    7    1    5    2]\n",
      " [   1    1    0    0    2    3  946    0    5    0]\n",
      " [   2    1   19    3    3    0    0  952   18   30]\n",
      " [   1    0    2    1    5    6    1    2  952    4]\n",
      " [   6    0    0    1   32    1    0    1    8  960]]\n"
     ]
    }
   ],
   "source": [
    "mnist_lstm_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 備註:lstm模型訓練中，增加epoch可以明顯提高準確度(95%->97%)，增加dense則沒有顯著效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2)CNN ，印出預測準確度以及Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 10, 10, 48)        38448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 5, 5, 48)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               307456    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 84)                21588     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 369,174\n",
      "Trainable params: 369,174\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 0.1374 - acc: 0.9578 - val_loss: 0.0546 - val_acc: 0.9818\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.0415 - acc: 0.9872 - val_loss: 0.0326 - val_acc: 0.9889\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.0295 - acc: 0.9907 - val_loss: 0.0349 - val_acc: 0.9882\n",
      "CNN test accuracy: 0.9882\n",
      "[[5907    0    0    1    0    3    0    1   11    0]\n",
      " [   4 6635    7    9    5    7    5   58    9    3]\n",
      " [   0    1 5931   13    0    1    0    5    6    1]\n",
      " [   0    0    2 6122    0    4    0    2    0    1]\n",
      " [   1    0    3    0 5755    0    2   31   11   39]\n",
      " [   0    0    0   11    0 5405    1    1    3    0]\n",
      " [   5    0    1    0    1   49 5853    0    9    0]\n",
      " [   0    0    2    3    0    0    0 6260    0    0]\n",
      " [   0    1    0   11    0   24    0    3 5810    2]\n",
      " [  11    0    0    5    6   10    0   21    4 5892]] \n",
      "\n",
      " [[ 975    0    0    0    0    2    0    2    1    0]\n",
      " [   2 1114    0    4    1    2    4    6    1    1]\n",
      " [   0    0 1023    3    0    0    0    5    1    0]\n",
      " [   0    0    0 1009    0    1    0    0    0    0]\n",
      " [   0    0    0    1  962    1    3    3    4    8]\n",
      " [   1    0    0    9    0  879    1    0    0    2]\n",
      " [   3    1    1    0    1   11  939    0    2    0]\n",
      " [   0    0    0    1    0    0    0 1026    1    0]\n",
      " [   0    0    1    5    0    2    0    1  964    1]\n",
      " [   0    0    0    4    2    4    0    4    4  991]]\n"
     ]
    }
   ],
   "source": [
    "mnist_cnn_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 備註:cnn模型訓練中，增加epoch並沒有明顯提高準確度"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
