{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3:Use LSTM & CNN model to classify customized candlestick pattern (at least 3 classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.程式碼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1)Use LSTM model to classify customized candlestick pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* candlestick_lstm_training_r09723032_江奕萱.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "import keras\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPool2D, Dropout, Flatten\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#讀取資料的部分\n",
    "def load_data(name):\n",
    "    # load data from data folder\n",
    "    with open(name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "'''\n",
    "資料前處理 Dataset\n",
    "'''\n",
    "def lstm_preprocess(x_train, x_test, y_train, y_test, n_step, n_input, n_classes):\n",
    "    x_train = x_train.reshape(-1, n_step, n_input)\n",
    "    x_test = x_test.reshape(-1, n_step, n_input)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255 #標準化數據\n",
    "    x_test /= 255 #標準化數據\n",
    "    y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, n_classes)\n",
    "    return (x_train, x_test, y_train, y_test)\n",
    "\n",
    "\n",
    "'''\n",
    "建立LSTM模型:\n",
    "主要是用一層LSTM以及一層的dense進行預測，\n",
    "output使用softmax作為計算的fuction\n",
    "\n",
    "模型調整:\n",
    "新增Dense(128, activation='relu')\n",
    "新增Dense(64, activation='relu')\n",
    "'''\n",
    "def lstm_model(n_input, n_step, n_hidden, n_classes):\n",
    "    #添加LSTM、Dense層\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_hidden, batch_input_shape=(None, n_step, n_input), unroll=True))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "'''\n",
    "訓練模型:\n",
    "先設定優化器Adam\n",
    "再來設定模型的Loss函數、優化器以及用來判斷模型好壞的依據（metrics），此處使用accuracy\n",
    "最後訓練模型\n",
    "'''\n",
    "def lstm_train(model, x_train, y_train, x_test, y_test,learning_rate, training_iters, batch_size):\n",
    "    adam = Adam(lr=learning_rate)\n",
    "    model.summary()\n",
    "    model.compile(optimizer=adam,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train,batch_size=batch_size, epochs=training_iters,verbose=1, validation_data=(x_test, y_test))\n",
    "'''\n",
    "辨別機器學習模型的好壞，印出Confusion Matrix:\n",
    "先得到模型的預測結果\n",
    "然後與真正的答案做對照\n",
    "由左上至右下的對角線，代表True Positive和True Negative，\n",
    "亦即模型預測成功的狀況，因此數字越高越好\n",
    "'''\n",
    "def lstm_result(data, x_train, x_test, model):\n",
    "    # get train & test pred-labels\n",
    "    train_pred = model.predict_classes(x_train)\n",
    "    test_pred = model.predict_classes(x_test)\n",
    "    # get train & test true-labels\n",
    "    train_label = data['train_label'][:, 0]\n",
    "    test_label = data['test_label'][:, 0]\n",
    "    # confusion matrix\n",
    "    train_result_cm = confusion_matrix(train_label, train_pred, labels=range(9))\n",
    "    test_result_cm = confusion_matrix(test_label, test_pred, labels=range(9))\n",
    "    print(train_result_cm, '\\n'*2, test_result_cm)\n",
    "\n",
    "'''\n",
    "補充說明:\n",
    "Batch_Size 設越大，\n",
    "則跑完一個 Epoch 的時間大約成比例縮小(因為要跑的 Iteration就比例減少)\n",
    "但設的太大，就必須放大 Epoch 數。\n",
    "這是因為Batch_Size大，\n",
    "一個 Epoch 可以跑得 Iteration 數就成比例變少，\n",
    "就沒有足夠的梯度下降讓損失函數到平穩的低點。\n",
    "所以必須加大 Epoch 數，這樣訓練時間又變長了，\n",
    "取捨之間也是必須用觀察的。\n",
    "\n",
    "參數調整:\n",
    "learning_rate : 0.001->0.01\n",
    "batch_size: 128->32\n",
    "'''\n",
    "def candlestick_lstm():\n",
    "    # training parameters\n",
    "    learning_rate = 0.01 #學習率\n",
    "    training_iters = 10\n",
    "    batch_size = 32 # BATCH的大小，相當於一次處理的個數\n",
    "\n",
    "    # model parameters\n",
    "    n_input = 40 \n",
    "    n_step = 10\n",
    "    n_hidden = 256 #隱含層的特徵數\n",
    "    n_classes = 10 \n",
    "\n",
    "    data = load_data('./data/label8_eurusd_10bar_1500_500_val200_gaf_culr.pkl')\n",
    "    x_train, y_train, x_test, y_test = data['train_gaf'], data['train_label'][:, 0], data['test_gaf'], data['test_label'][:, 0]\n",
    "    x_train, x_test, y_train, y_test = lstm_preprocess(x_train, x_test, y_train, y_test, n_step, n_input, n_classes)\n",
    "\n",
    "    model = lstm_model(n_input, n_step, n_hidden, n_classes)\n",
    "    lstm_train(model, x_train, y_train, x_test, y_test, learning_rate,training_iters, batch_size)\n",
    "    scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('LSTM test accuracy:', scores[1])\n",
    "    lstm_result(data, x_train, x_test, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2)Use CNN model to classify customized candlestick pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* candlestick_cnn_training_r09723032_江奕萱.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, Activation, MaxPool2D\n",
    "\n",
    "\n",
    "#讀取資料\n",
    "def load_data(name):\n",
    "    # load data from data folder\n",
    "    with open(name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "'''\n",
    "建立CNN模型：\n",
    "先建立了 Convolution 層(兩層)，\n",
    "接著用 Flattern 攤平維度，\n",
    "然後接 Dense 全連接層三層，\n",
    "最後輸出9個類別\n",
    "'''\n",
    "# Model Structure\n",
    "def cnn_model(params):\n",
    "    # initializing CNN\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(10, 10, 4)))\n",
    "    # Second convolutional layer\n",
    "    model.add(Conv2D(filters=48, kernel_size=(5,5), padding='valid', activation='relu')) \n",
    "    #將 feature maps 攤平放入一個向量中\n",
    "    model.add(Flatten()) #攤平維度\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(84, activation='relu'))\n",
    "    model.add(Dense(9, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "    \n",
    "'''\n",
    "訓練模型：\n",
    "選擇使用CNN Model\n",
    "設定模型的Loss函數、優化器以及用來判斷模型好壞的依據（metrics），這裡用準確度來衡量\n",
    "最後訓練模型\n",
    "'''\n",
    "def cnn_train(params, data):\n",
    "    model = cnn_model(params)\n",
    "    # 設定模型的Loss函數、優化器以及用來判斷模型好壞的依據（metrics）\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=params['optimizer'], metrics=['accuracy'])\n",
    "    #訓練模型\n",
    "    hist = model.fit(x=data['train_gaf'], y=data['train_label_arr'],batch_size=params['batch_size'], epochs=params['epochs'], verbose=2)\n",
    "    return (model, hist)\n",
    "\n",
    "'''\n",
    "辨別機器學習模型的好壞，印出Confusion Matrix：\n",
    "先得到模型的預測結果\n",
    "然後與真正的答案做對照\n",
    "由左上至右下的對角線，代表True Positive和True Negative，\n",
    "亦即模型預測成功的狀況，因此數字越高越好\n",
    "'''\n",
    "\n",
    "def cnn_result(data, model):\n",
    "    # get train & test pred-labels\n",
    "    train_pred = model.predict_classes(data['train_gaf'])\n",
    "    test_pred = model.predict_classes(data['test_gaf'])\n",
    "    # get train & test true-labels\n",
    "    train_label = data['train_label'][:, 0]\n",
    "    test_label = data['test_label'][:, 0]\n",
    "    # confusion matrix\n",
    "    train_result_cm = confusion_matrix(train_label, train_pred, labels=range(9))\n",
    "    test_result_cm = confusion_matrix(test_label, test_pred, labels=range(9))\n",
    "    print(train_result_cm, '\\n'*2, test_result_cm)\n",
    "\n",
    "'''\n",
    "補充說明:\n",
    "Batch_Size 設越大，\n",
    "則跑完一個 Epoch 的時間大約成比例縮小(因為要跑的 Iteration就比例減少)\n",
    "但設的太大，就必須放大 Epoch 數。\n",
    "這是因為Batch_Size大，\n",
    "一個 Epoch 可以跑得 Iteration 數就成比例變少，就\n",
    "沒有足夠的梯度下降讓損失函數到平穩的低點。\n",
    "所以必須加大 Epoch 數，這樣訓練時間又變長了，\n",
    "取捨之間也是必須用觀察的。\n",
    "\n",
    "參數調整:\n",
    "learning_rate : 0.01->0.05\n",
    "batch_size: 64->32\n",
    "'''\n",
    "def candlestick_cnn():\n",
    "    PARAMS = {}\n",
    "    PARAMS['pkl_name'] = './data/label8_eurusd_10bar_1500_500_val200_gaf_culr.pkl' \n",
    "    # training parameters \n",
    "    PARAMS['classes'] = 9 \n",
    "    PARAMS['lr'] = 0.05  #學習率\n",
    "    PARAMS['epochs'] = 10  #決定訓練要跑幾回合 epoch，一個 epoch 就是全部的訓練數據都被掃過一遍。\n",
    "    PARAMS['batch_size'] = 32 # BATCH的大小，相當於一次處理的個數\n",
    "    PARAMS['optimizer'] = optimizers.SGD(lr=PARAMS['lr'])\n",
    "    # load data & keras model\n",
    "    data = load_data(PARAMS['pkl_name'])\n",
    "    # train cnn model\n",
    "    model, hist = cnn_train(PARAMS, data)\n",
    "    # 驗證模型\n",
    "    scores = model.evaluate(data['test_gaf'], data['test_label_arr'], verbose=0)\n",
    "    print('CNN test accuracy:', scores[1])\n",
    "    cnn_result(data, model)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.執行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1)LSTM ，印出預測準確度以及Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 256)               304128    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 345,930\n",
      "Trainable params: 345,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 22s 33ms/step - loss: 1.7555 - accuracy: 0.3209 - val_loss: 0.7226 - val_accuracy: 0.7244\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.6588 - accuracy: 0.7425 - val_loss: 0.4975 - val_accuracy: 0.8184\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.5272 - accuracy: 0.8019 - val_loss: 0.4611 - val_accuracy: 0.8266\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.4760 - accuracy: 0.8196 - val_loss: 0.4287 - val_accuracy: 0.8470\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.4688 - accuracy: 0.8294 - val_loss: 0.3870 - val_accuracy: 0.8616\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.4353 - accuracy: 0.8365 - val_loss: 0.3783 - val_accuracy: 0.8646\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.4114 - accuracy: 0.8513 - val_loss: 0.4058 - val_accuracy: 0.8518\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.4018 - accuracy: 0.8569 - val_loss: 0.3249 - val_accuracy: 0.8912\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.3858 - accuracy: 0.8627 - val_loss: 0.3274 - val_accuracy: 0.8868\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.3787 - accuracy: 0.8644 - val_loss: 0.3523 - val_accuracy: 0.8738\n",
      "LSTM test accuracy: 0.8737999796867371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ben82\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2312  114   73  122   71   86   55   97   70]\n",
      " [  12 1483    0    5    0    0    0    0    0]\n",
      " [ 110    0 1382    0    8    0    0    0    0]\n",
      " [  25    8    0 1363    0    0    0  104    0]\n",
      " [ 142    0    9    0 1318    0    0    0   31]\n",
      " [  83    0    0    0    0 1400    0   17    0]\n",
      " [ 154    1    1    0    3    0 1325    0   16]\n",
      " [  22    0    0  140    0    8    0 1330    0]\n",
      " [ 101    0    2    0  177    0   29    0 1191]] \n",
      "\n",
      " [[762  42  27  35  23  32  15  39  25]\n",
      " [  9 489   0   2   0   0   0   0   0]\n",
      " [ 30   0 469   0   1   0   0   0   0]\n",
      " [  9  10   0 442   0   0   0  39   0]\n",
      " [ 46   0  10   0 433   0   0   0  11]\n",
      " [ 33   0   0   0   0 464   0   3   0]\n",
      " [ 65   0   1   0   0   0 431   0   3]\n",
      " [  7   1   0  32   0   2   0 458   0]\n",
      " [ 21   0   0   0  51   0   7   0 421]]\n"
     ]
    }
   ],
   "source": [
    "candlestick_lstm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 調整模型和參數之後，準確度提高:80%->87%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2)CNN ，印出預測準確度以及Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 - 7s - loss: 0.8790 - accuracy: 0.6592\n",
      "Epoch 2/10\n",
      "469/469 - 6s - loss: 0.5205 - accuracy: 0.8042\n",
      "Epoch 3/10\n",
      "469/469 - 6s - loss: 0.4529 - accuracy: 0.8301\n",
      "Epoch 4/10\n",
      "469/469 - 6s - loss: 0.4182 - accuracy: 0.8479\n",
      "Epoch 5/10\n",
      "469/469 - 6s - loss: 0.3872 - accuracy: 0.8567\n",
      "Epoch 6/10\n",
      "469/469 - 6s - loss: 0.3585 - accuracy: 0.8676\n",
      "Epoch 7/10\n",
      "469/469 - 6s - loss: 0.3316 - accuracy: 0.8783\n",
      "Epoch 8/10\n",
      "469/469 - 6s - loss: 0.3122 - accuracy: 0.8875\n",
      "Epoch 9/10\n",
      "469/469 - 6s - loss: 0.2958 - accuracy: 0.8920\n",
      "Epoch 10/10\n",
      "469/469 - 6s - loss: 0.2714 - accuracy: 0.9013\n",
      "CNN test accuracy: 0.8809999823570251\n",
      "[[2476   75   58   76   58   42  100   80   35]\n",
      " [   4 1496    0    0    0    0    0    0    0]\n",
      " [  12    0 1488    0    0    0    0    0    0]\n",
      " [   7   12    0 1419    0    0    0   62    0]\n",
      " [  30    0   31    0 1379    0    1    0   59]\n",
      " [  49    0    0    1    0 1428    0   22    0]\n",
      " [   7    0    0    0    0    0 1490    0    3]\n",
      " [   4    1    0   92    0    1    0 1402    0]\n",
      " [  26    0    2    0   77    0   30    0 1365]] \n",
      "\n",
      " [[744  32  37  29  23  30  44  34  27]\n",
      " [  8 491   0   1   0   0   0   0   0]\n",
      " [ 11   0 489   0   0   0   0   0   0]\n",
      " [ 17  13   0 445   0   0   0  25   0]\n",
      " [ 33   0  30   0 411   0   0   0  26]\n",
      " [ 52   0   0   0   0 441   0   7   0]\n",
      " [ 19   0   1   0   0   0 479   0   1]\n",
      " [ 13   1   0  16   0   1   0 469   0]\n",
      " [ 16   0   0   0  29   0  19   0 436]]\n"
     ]
    }
   ],
   "source": [
    "candlestick_cnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 調整參數之後，準確度提高:80%->88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
